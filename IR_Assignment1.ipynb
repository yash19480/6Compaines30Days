{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP_H6AqKwG9W"
      },
      "source": [
        "# Ques 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDLMTCj1wG9c",
        "outputId": "e2797f6a-229f-4fda-907f-30c67529f26d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize.casual import EMOTICON_RE\n",
        "from nltk.tokenize import RegexpTokenizer as reg\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "path = '/content/drive/MyDrive/Dataset/Humor,Hist,Media,Food'\n",
        "file_names = []\n",
        "all_txt = []\n",
        "dataset = []\n",
        "\n",
        "for f in os.listdir(path):\n",
        "    all_txt.append(os.path.join(path, f))\n",
        "    file_names.append(f)\n",
        "\n",
        "for i in all_txt:\n",
        "    dataset.append(open(i,'r',encoding=\"iso-8859-1\").read())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5Xn93h3wUwv",
        "outputId": "992f7c20-7524-477e-d849-cc8f7fc5e22a"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data) : \n",
        "    preprocessed = []\n",
        "    cache = set(stopwords.words(\"english\"))\n",
        "    porter = PorterStemmer()\n",
        "\n",
        "    for file in data:\n",
        "        preproc = re.sub('[^a-zA-Z0-9]',' ',file)\n",
        "        preproc = preproc.lower()\n",
        "        preproc = preproc.split()\n",
        "        preproc = [porter.stem(word) for word in preproc if not word in cache]\n",
        "        preprocessed.append(preproc)\n",
        "        \n",
        "    return preprocessed"
      ],
      "metadata": {
        "id": "pgzG3SrXw8gM"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unigram_index(data) : \n",
        "    sol={}\n",
        "    all_docs = []\n",
        "    doc_idx = 0\n",
        "    for text in (data) : \n",
        "        # print(text)\n",
        "        for word in text :\n",
        "            # print(word) \n",
        "            if word in sol : \n",
        "                if(doc_idx in sol[word]) : \n",
        "                    continue\n",
        "                else : \n",
        "                    sol[word].append(doc_idx)\n",
        "            else : \n",
        "                sol[word] = []\n",
        "                sol[word].append(doc_idx)\n",
        "        all_docs.append(doc_idx)\n",
        "        doc_idx += 1\n",
        "    return sol, all_docs"
      ],
      "metadata": {
        "id": "ZrwXdZrW2LvV"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = preprocess(dataset)\n",
        "dict_unigram = {}\n",
        "dict_unigram, all_docs = unigram_index(preprocessed_data)"
      ],
      "metadata": {
        "id": "qFAwCC9Tw8fS"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def and_op(doc1, doc2) : \n",
        "    doc1.sort()\n",
        "    doc2.sort()\n",
        "\n",
        "    ans = []\n",
        "    tot = 0\n",
        "    i=0\n",
        "    j=0\n",
        "\n",
        "    while(i<len(doc1) and j<len(doc2)) :\n",
        "        if(doc1[i] == doc2[j]) :\n",
        "            ans.append(doc1[i]) \n",
        "            i+=1\n",
        "            j+=1\n",
        "        elif(doc1[i] < doc2[j]) :\n",
        "            i+=1\n",
        "        else :\n",
        "            j += 1\n",
        "        tot += 1\n",
        "    \n",
        "    return ans, tot"
      ],
      "metadata": {
        "id": "2wgKMZh9w8P2"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def or_op(doc1, doc2):\n",
        "    doc1.sort()\n",
        "    doc2.sort()\n",
        "    i=0\n",
        "    j=0\n",
        "    ans = []\n",
        "    tot = 0\n",
        "    while(i<len(doc1) and j<len(doc2)):\n",
        "        if(doc1[i] == doc2[j]):\n",
        "            ans.append(doc1[i])\n",
        "            i+=1\n",
        "            j+=1\n",
        "        elif(doc1[i] < doc2[j]):\n",
        "            ans.append(doc1[i])\n",
        "            i+=1\n",
        "        else:\n",
        "            ans.append(doc2[j])\n",
        "            j+=1\n",
        "        tot+=1\n",
        "\n",
        "    if(i == len(doc1)):\n",
        "        while(j<len(doc2)):\n",
        "            ans.append(doc2[j])\n",
        "            j+=1\n",
        "        \n",
        "    if(j == len(doc2)) : \n",
        "        while(i<len(doc1)):\n",
        "            ans.append(doc1[i])\n",
        "            i+=1\n",
        "    \n",
        "    return ans, tot\n",
        "        "
      ],
      "metadata": {
        "id": "5ZMDsMA65vXQ"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def not_op(doc1, doc2) : \n",
        "    return list(set(doc1)-set(doc2))"
      ],
      "metadata": {
        "id": "O3uO-gslz-hV"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_query(q) : \n",
        "    porter = PorterStemmer()\n",
        "    preproc = re.sub('[^a-zA-Z0-9]',' ',q)\n",
        "    preproc = preproc.lower()\n",
        "    preproc = preproc.split()\n",
        "    preproc = [porter.stem(word) for word in preproc]\n",
        "\n",
        "    return preproc\n",
        "n = 1 \n",
        "\n",
        "queries = []\n",
        "for i in range(n) : \n",
        "    query_inp = input()\n",
        "    # query_inp = \"telephone,paved, roads\"\n",
        "    query_inp = preprocess_query(query_inp)\n",
        "    input_seq = []\n",
        "    for j in range(len(query_inp)-1): \n",
        "        inp = input()\n",
        "        input_seq.append(inp)\n",
        "    query = []\n",
        "    for j in range(0, len(query_inp)) :\n",
        "        query.append(query_inp[j])\n",
        "        # print(j)\n",
        "        # print(len(input_seq))\n",
        "        if(j < len(input_seq)) : \n",
        "            x = input_seq[j].lower()\n",
        "            x = x.split()\n",
        "            query.extend(x)\n",
        "    queries.append(query)\n",
        "    # print(query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poWdndNct7jo",
        "outputId": "eb1bf5ee-e018-4fea-c30f-4202caaf8236"
      },
      "execution_count": 153,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "essay thesis readers\n",
            "AND\n",
            "OR NOT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for query in queries :\n",
        "    i=1\n",
        "\n",
        "    if(query[0] not in dict_unigram) : \n",
        "        dict_unigram[query[0]] = {}\n",
        "    prev = dict_unigram.get(query[0])\n",
        "    tot = 0\n",
        "    while(i < len(query)-1) : \n",
        "        if(query[i] == 'and') : \n",
        "            if(i+1 < len(query)-1 and query[i+1] == 'not') : \n",
        "                if(not(query[i+2] in dict_unigram)):\n",
        "                    dict_unigram[query[i+2]]={}\n",
        "                a2 = not_op(all_docs, dict.unigram(query[i+2]))\n",
        "                v1, v2 = and_op(prev, a2)\n",
        "                tot += v2\n",
        "                prev = v1\n",
        "                i+=3\n",
        "            else : \n",
        "                a2 = dict_unigram.get(query[i+1])\n",
        "                v1, v2 = and_op(prev, a2)\n",
        "                tot += v2\n",
        "                prev = v1\n",
        "                i+=2\n",
        "        \n",
        "        elif(query[i] == 'or') : \n",
        "            if(i+1 < len(query)-1 and query[i+1] == 'not') : \n",
        "                if(not(query[i+2] in dict_unigram)):\n",
        "                    dict_unigram[query[i+2]]={}\n",
        "                a2 = not_op(all_docs, dict_unigram.get(query[i+2]))\n",
        "                # a2 = list(set(all_docs)-set(dict_unigram.get(query[i+2])))\n",
        "                v1, v2 = or_op(prev, a2)\n",
        "                # print(\"v1 \", v1) \n",
        "                tot += v2\n",
        "                prev = v1\n",
        "                i+=3\n",
        "            else : \n",
        "                # query[i+1] = preproc_word(query[i+1])\n",
        "                a2 = dict_unigram.get(query[i+1])\n",
        "                # print(prev)\n",
        "                # print(a2)\n",
        "                v1, v2 = or_op(prev, a2)\n",
        "                print(\"v1 \", v1)\n",
        "                tot += v2\n",
        "                prev = v1\n",
        "                i+=2\n",
        "\n",
        "    print(\"Total Number of Documents \", len(prev))\n",
        "    print(\"Total number of comparisons \", tot)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwC3zulQ5vSE",
        "outputId": "8c5520fb-822b-41d5-b210-59ea7032a092"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Documents  1051\n",
            "Total number of comparisons  41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ySxD4MQE09K3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "IR_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}